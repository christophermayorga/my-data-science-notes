{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T21:27:02.170852Z",
     "start_time": "2020-10-23T21:26:57.475326Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import csv\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Required Libraries\n",
    "\n",
    "#Base and Cleaning \n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls\n",
    "\n",
    "#Natural Language Processing (NLP)\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "import datapane as dp \n",
    "\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from [here](https://datapane.com/u/khuyentran1401/reports/tweets/) then put the data in your current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T21:27:02.473854Z",
     "start_time": "2020-10-23T21:27:02.172004Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('dp-export-8927.csv') #Change this to the name of the csv file you downloaded\n",
    "\n",
    "tweets = tweets.Tweets.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T21:27:08.055956Z",
     "start_time": "2020-10-23T21:27:08.048149Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:58:23.014574Z",
     "start_time": "2020-10-22T20:58:23.011193Z"
    }
   },
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "    #match url (i.e: https://t.co/5tF5G9VKtq)\n",
    "    (r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ''),\n",
    "\n",
    "    #match user (i.e: @cerpintor )\n",
    "    (r'@\\w+', ''),\n",
    "\n",
    "    #match hashtag (i.e: #WomensMarchOnWashington)\n",
    "    (r'#\\w+', ''),\n",
    "\n",
    "    #Replace \"&...\" with ''\n",
    "    (r'&\\w+', '')\n",
    "]\n",
    "\n",
    "class RegexReplacer(object):\n",
    "    def __init__(self, patterns = replacement_patterns):\n",
    "        self.patterns = [(re.compile(regrex),repl) for (regrex, repl) in\n",
    "                        patterns]\n",
    "    \n",
    "    #Replace the words that match the patterns with replacement words\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:58:22.374105Z",
     "start_time": "2020-10-22T20:58:22.371574Z"
    }
   },
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:07:49.748160Z",
     "start_time": "2020-10-22T21:07:49.736157Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    \n",
    "    doc = nlp(\" \".join(text)) \n",
    "    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:02:47.478093Z",
     "start_time": "2020-10-22T21:02:47.042515Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "tknz = TweetTokenizer()\n",
    "replacer = RegexReplacer()\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:04:58.301401Z",
     "start_time": "2020-10-22T21:03:18.465815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tweets, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tweets], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:08:13.414167Z",
     "start_time": "2020-10-22T21:08:12.833832Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(doc):\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        doc[i] = give_emoji_free_text(doc[i])\n",
    "        \n",
    "        #Tokenize with replacement\n",
    "        doc[i] = tknz.tokenize(replacer.replace(doc[i]))\n",
    "        \n",
    "        #Filter stopwords, punctuations, and lowercase\n",
    "        doc[i] = [w.lower() for w in doc[i] if w not in punc and w not in ALL_STOP_WORDS]\n",
    "    \n",
    "        # Bigram\n",
    "        doc[i] = bigram_mod[doc[i]]\n",
    "        \n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        doc[i] = lemmatization(doc[i], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        \n",
    "        \n",
    "        #concat\n",
    "#         doc[i] = ' '.join(w for w in doc[i])\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:11:33.436179Z",
     "start_time": "2020-10-22T21:08:17.034854Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = normalize(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:11:33.448200Z",
     "start_time": "2020-10-22T21:11:33.437914Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T02:56:13.427654Z",
     "start_time": "2020-10-23T02:56:13.266977Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_tweets = [','.join(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T02:56:32.318365Z",
     "start_time": "2020-10-23T02:56:20.253603Z"
    }
   },
   "outputs": [],
   "source": [
    "# dp.Report(dp.Table(pd.DataFrame(join_tweets, columns=['Tweets']))).publish(name='processed_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
